{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acWc6PRjtRYG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "J8nHyPNutb_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculating average text embeddings of sample questions for each domain"
      ],
      "metadata": {
        "id": "qFoNYBtxu5-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
        "dataset_path = 'biomed_dataset' # sample questions from domains dataset"
      ],
      "metadata": {
        "id": "k4iU1Z9Ftedc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(checkpoint)"
      ],
      "metadata": {
        "id": "RuRL7UDatc19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_path, split='train')\n",
        "bio = dataset.filter(lambda example: example['domain'] == 'bio')\n",
        "fin = dataset.filter(lambda example: example['domain'] == 'fin')\n",
        "legal = dataset.filter(lambda example: example['domain'] == 'legal')"
      ],
      "metadata": {
        "id": "RJl-BHyxtqRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_embedding = model.encode(bio['question'])\n",
        "fin_embedding = model.encode(fin['question'])\n",
        "legal_embedding = model.encode(legal['question'])\n",
        "bio_avg = np.mean(m_embedding, axis=0)\n",
        "fin_avg = np.mean(f_embedding, axis=0)\n",
        "legal_avg = np.mean(l_embedding, axis=0)"
      ],
      "metadata": {
        "id": "aN7fLQStuufG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('bio.npy', 'wb') as f:\n",
        "  np.save(f, bio_avg)\n",
        "with open('fin.npy', 'wb') as f:\n",
        "  np.save(f, fin_avg)\n",
        "with open('legal.npy', 'wb') as f:\n",
        "  np.save(f, legal_avg)"
      ],
      "metadata": {
        "id": "QK2G9Z8avE-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test domain selection accuracy"
      ],
      "metadata": {
        "id": "HqXQMROYvdzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = 0 # 0 for bio, 1 for finance, 2 for legal\n",
        "test_domain = 'bio' # 'bio', 'fin' or 'legal'\n",
        "test_size = 100 # number of test questions\n",
        "\n",
        "with open(\"bio-avg.npy\", 'rb') as f:\n",
        "  bio_avg = np.load(f)\n",
        "with open(\"fin-avg.npy\", 'rb') as f:\n",
        "  fin_avg = np.load(f)\n",
        "with open(\"legal-avg.npy\", 'rb') as f:\n",
        "  legal_avg = np.load(f)"
      ],
      "metadata": {
        "id": "qYsEYtguw0e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = load_dataset(dataset_path, split='test')\n",
        "sub_test = test.filter(lambda example: example['domain'] == test_domain)\n",
        "sub_test = sub_test.select(range(test_size))"
      ],
      "metadata": {
        "id": "0n6aXB2_wrhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_domain(q, model, bio_avg, fin_avg, legal_avg):\n",
        "  embedding = model.encode(q)\n",
        "  bio = [util.cos_sim(embedding, bio_avg).numpy()[0][0]]\n",
        "  fin = [util.cos_sim(embedding, fin_avg).numpy()[0][0]]\n",
        "  legal = [util.cos_sim(embedding, legal_avg).numpy()[0][0]]\n",
        "  similarity_scores = [bio, fin, legal]\n",
        "  index = np.argmax(similarity_scores, axis=0)[0]\n",
        "  return index # returns the index of selected domain 0, 1 or 2 respectively from similarity score list"
      ],
      "metadata": {
        "id": "jKt2lF5evi2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = []\n",
        "for i in range(len(test_size)):\n",
        "  question = sub_test[i]['question']\n",
        "  pred_domain_index = select_domain(question, model, bio_avg, fin_avg, legal_avg)\n",
        "  if (pred_domain_index == label):\n",
        "    accuracy.append(1)\n",
        "  else:\n",
        "    accuracy.append(0)\n",
        "\n",
        "#print(\"Accuracy in domain X: \" + str(np.mean(accuracy)))"
      ],
      "metadata": {
        "id": "bNoO7YLcwtMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}