{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on17IFCjh5W8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install adapters datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import required"
      ],
      "metadata": {
        "id": "utZxuncrjfz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments\n",
        "from adapters import AdapterTrainer, AdapterConfig, init"
      ],
      "metadata": {
        "id": "NTXlgp3niGk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training task adapters"
      ],
      "metadata": {
        "id": "mDvkSwiYjiJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "HF_KEY = \"hf_xxx\"\n",
        "hf_dataset_path = \"xxx\"\n",
        "output_dir = \"./output_dir\"\n",
        "hf_domain_adapter_path = \"xxx\"\n",
        "adapter_save_name = \"xxx\"\n",
        "hf_adapter_upload_path = \"xxx\"\n",
        "\n",
        "adapter_type = \"pfeiffer\"\n",
        "lr = 1e-4\n",
        "num_epochs = 5\n",
        "r_factor = 16"
      ],
      "metadata": {
        "id": "jZwGRgiMiy75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "login(HF_KEY)\n",
        "checkpoint = \"gpt2-medium\""
      ],
      "metadata": {
        "id": "Q3cphTgMizPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset = load_dataset(hf_dataset_path, split=\"train\")\n",
        "qa_dataset = qa_dataset.shuffle(SEED)\n",
        "qa_dataset = qa_dataset.train_test_split(test_size=0.15)"
      ],
      "metadata": {
        "id": "sJfZoqvnjJLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(examples):\n",
        "  examples[\"text\"] = \"Question: \" + examples[\"question\"] + \"Answer: \" + examples[\"answer\"]\n",
        "  return examples\n",
        "qa_dataset = qa_dataset.map(preprocess)"
      ],
      "metadata": {
        "id": "5cw4N2UzoL-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "JrogQqHZjeIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_batch(batch):\n",
        "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
        "  encoding = tokenizer(batch[\"text\"], truncation=True, max_length=256)\n",
        "  return encoding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "column_names = qa_dataset[\"train\"].column_names\n",
        "qa_data = qa_dataset.map(encode_batch, remove_columns=column_names, batched=True)"
      ],
      "metadata": {
        "id": "0Xmu-V0Ajrpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 64\n",
        "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
        "def group_texts(examples):\n",
        "  # Concatenate all texts.\n",
        "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "  # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "  # customize this part to your needs.\n",
        "  total_length = (total_length // block_size) * block_size\n",
        "  # Split by chunks of max_len.\n",
        "  result = {\n",
        "    k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "  }\n",
        "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "  return result\n",
        "\n",
        "qa_data = qa_data.map(group_texts,batched=True,)\n",
        "qa_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {
        "id": "uA-Wm13Fj2K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "init(model)\n",
        "model.load_adapter(hf_adapter_path, load_as='domain', with_head=False)"
      ],
      "metadata": {
        "id": "vwepBcm1kDu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_config = AdapterConfig.load(adapter_type, reduction_factor=r_factor)\n",
        "model.add_adapter(\"task\", config=adapter_config)\n",
        "model.train_adapter(\"task\")\n",
        "model.active_adapters = Stack('domain', 'task')\n",
        "model.adapter_to(device)\n",
        "# print(model.adapter_summary())"
      ],
      "metadata": {
        "id": "TPMoOFv2k--A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir= output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    do_train=True,\n",
        "    remove_unused_columns=False,\n",
        "    learning_rate=lr,\n",
        "    num_train_epochs=num_epochs,\n",
        "    report_to=\"none\",\n",
        "    )\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=qa_data[\"train\"],\n",
        "    eval_dataset=qa_data[\"test\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "    )"
      ],
      "metadata": {
        "id": "dYVJq7RblaA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "l-oeASOdmsbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_result['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "ZEQDqAAumvJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_adapter_to_hub(hf_adapter_upload_path, adapter_name=adapter_save_name)\n",
        "# model.save_adapter(\"legal_domain\", \"domain\")"
      ],
      "metadata": {
        "id": "qaFEVZxdm3yU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}