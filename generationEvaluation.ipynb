{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUgeYjgzTfM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install adapters datasets evaluate bert_score rouge_score sacremoses sacrebleu openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from huggingface_hub import login\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from adapters import init\n",
        "from adapters.composition import Stack\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from evaluate import load\n",
        "from openai import OpenAI\n",
        "import os"
      ],
      "metadata": {
        "id": "eVuIUYTDzfdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "modelpath = \"gpt2-medium\"\n",
        "domain_adapter = \"hf_path_to_trained_DA\"\n",
        "task_adapter = \"hf_path_to_trained_TA\"\n",
        "HF_KEY = \"hf_key\"\n",
        "key = \"openai_key\"\n",
        "test_dataset = \"hf_path_to_test_dataset\"\n",
        "test_size = 200\n",
        "\n",
        "# generation params\n",
        "temperature = 0.1\n",
        "rp = 1.03\n",
        "max_new_tokens = 100"
      ],
      "metadata": {
        "id": "4izPdRaXztZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "login(HF_KEY)"
      ],
      "metadata": {
        "id": "EtxHKnXR1WOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = load_dataset(test_dataset, split=\"train\")\n",
        "test.shuffle(SEED)\n",
        "test = test.select(range(test_size))"
      ],
      "metadata": {
        "id": "4bCopEd0zt-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(modelpath).to(DEVICE)\n",
        "init(model)"
      ],
      "metadata": {
        "id": "gQJ9R_Jo1kht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_adapter(domain_adapter, load_as=\"domain\", with_head=False)\n",
        "model.load_adaptert(task_adapter, load_as=\"task\", with_head=True)\n",
        "model.active_adapters = Stack(\"domain\",\"task\")\n",
        "model.adapter_to(DEVICE)\n",
        "# print(model.adapter_summary())"
      ],
      "metadata": {
        "id": "Orka5o1-0aru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(text: str):\n",
        "  encoding = tokenizer('Question: ' + text + \"Answer: \", return_tensors=\"pt\").to(device)\n",
        "  input_ids = encoding.input_ids\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    outputs = model.generate(input_ids, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, repetition_penalty= rp)\n",
        "  return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "opESSrAz032J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_answers = []\n",
        "reference_answers = []\n",
        "questions = []\n",
        "contexts = []\n",
        "for i in tqdm(range(test_size)):\n",
        "  reference_answers.append(test[i]['answer'])\n",
        "  questions.append(test[i]['question'])\n",
        "  contexts.append(test[i]['context'])\n",
        "  text = generate_answer(test[i]['question'])\n",
        "  text = text.split(\"Answer:\", 1)[1]\n",
        "  result = text[:text.rfind('.') + 1] or text # extract the answer until last complete sentence\n",
        "  generated_answers.append(result)"
      ],
      "metadata": {
        "id": "VJj4TlQx1nk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_result = pd.DataFrame({'question': questions, 'generated': generated_answers, 'reference': reference_answers})\n",
        "test_result.to_csv('eval_outputs.csv', index=False)"
      ],
      "metadata": {
        "id": "q5-5RxHu2eLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rouge\n"
      ],
      "metadata": {
        "id": "jcP8HBs-6FRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = load(\"rouge\")\n",
        "result = r.compute(predictions=test_result['generated'], references=test_result['reference'])\n",
        "test_result[\"r1\"] = result['rouge1']\n",
        "test_result['rL'] = result['rougeL']\n",
        "# print(np.mean(test_result[\"r1\"]))"
      ],
      "metadata": {
        "id": "4NClFKTb6GSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bleu"
      ],
      "metadata": {
        "id": "VZuEBEue64Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = load(\"bleu\")\n",
        "bleu_scores = []\n",
        "for x in range(100):\n",
        "  result_bleu = bleu.compute(predictions=[test_result['prediction'][x]], references=[test_result['reference'][x]])\n",
        "  bleu_scores.append(result_bleu['precisions'][0])\n",
        "test_result['bleu'] = bleu_scores"
      ],
      "metadata": {
        "id": "seX5T0nR65g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bertscore"
      ],
      "metadata": {
        "id": "9gUifApB5-9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertscore = load(\"bertscore\")\n",
        "result = bertscore.compute(predictions=test_result['generated'], references=test_result['reference'], model_type=\"distilbert-base-uncased\")\n",
        "test_result[\"bert_scores\"] = result['f1']\n",
        "# print(np.mean(test_result[\"bert_scores\"]))"
      ],
      "metadata": {
        "id": "LNx5YrUk26NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT4 - zero shot"
      ],
      "metadata": {
        "id": "8h1cc8aI6Hx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = key\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "baOPxePI3kxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarity(q, c, r, gen):\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    seed=42,\n",
        "    temperature=0,\n",
        "    max_tokens=50,\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are evaluating generated text using 0 to 10, in three scoring objective: first score indicating relevancy of generated answer to the question, second score indicating how much of generated text can be supported by provided context or reference answer, third score indicating factuality evaluation based on your knowledge regardless of the refence. Give the scores, and do not explain.\"},\n",
        "      {\"role\": \"user\", \"content\": f\"Question: '{q}' \\nContext: '{c}' \\nReference Answer: '{r} \\nGenerated Answer: '{gen}'\"}\n",
        "    ]\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "ijKUX4D_3wdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_scores = []\n",
        "for x in tqdm(range(test_size)):\n",
        "  gpt_scores.append(get_similarity(test_result['question'][x], test_result['context'][x], test_result['reference'][x], test_result['generated'][x]))"
      ],
      "metadata": {
        "id": "sCvMSJW84HjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = []\n",
        "s2 = []\n",
        "s3 = []\n",
        "for i in gpt_scores:\n",
        "  x = i.split('\\n')\n",
        "  x1 = x[0]\n",
        "  x2 = x[1]\n",
        "  x3 = x[2]\n",
        "  s1.append(float(x1.split(':',1)[1]))\n",
        "  s2.append(float(x2.split(':',1)[1]))\n",
        "  s3.append(float(x3.split(':',1)[1]))\n",
        "# print(np.mean(s1))"
      ],
      "metadata": {
        "id": "PHDxizO54Thc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_result['gpt4_relevance'] = s1\n",
        "test_result['gpt4_support'] = s2\n",
        "test_result['gpt4_factuality'] = s3"
      ],
      "metadata": {
        "id": "uKFrbjQG5D-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FactCC - example"
      ],
      "metadata": {
        "id": "C9aRlkDR6MLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pipeline(model=\"manueldeprada/FactCC\")"
      ],
      "metadata": {
        "id": "MN_vr7PW5ycW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "fact_cc = []\n",
        "for i in tqdm(range(100)):\n",
        "  gen = results['generated'][i]\n",
        "  gold = results['context'][i]\n",
        "  result = pipe([[[gold, gen]]], truncation='only_first', padding='max_length')\n",
        "  if result[0]['label'] == 'CORRECT':\n",
        "    fact_cc.append(1)\n",
        "  else:\n",
        "    fact_cc.append(0)\n",
        "test_result['fact_cc'] = fact_cc"
      ],
      "metadata": {
        "id": "PlbCHWTK57Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_result.to_csv('eval_with_scores.csv', index=False)"
      ],
      "metadata": {
        "id": "7baISVSb5KjZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}