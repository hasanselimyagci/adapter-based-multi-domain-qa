{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwa0hZPWxh55"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install adapters accelerate bitsandbytes datasets torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import required"
      ],
      "metadata": {
        "id": "MEeXkEk4Sgg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset\n",
        "from transformers import set_seed\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from adapters import init, AdapterConfig\n",
        "from adapters import AdapterTrainer\n",
        "from adapters.composition import Stack\n",
        "from transformers import DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "iMfFF0a8xwI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training domain adapter with quantized Llama 3.2 - 1B model\n",
        "\n",
        "Adapterhub notebook on finetuning Llama QLoRA used as reference:\n",
        "\n",
        "https://github.com/adapter-hub/adapters/blob/main/notebooks/QLoRA_Llama_Finetuning.ipynb"
      ],
      "metadata": {
        "id": "lszvIYKISnOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "HF_KEY = 'hf_key'\n",
        "datasetpath = 'hf_dataset_path'\n",
        "modelpath = 'base_model_path'\n",
        "adapter_type = 'pfeiffer'\n",
        "r_factor = 16\n",
        "output_dir = 'output_dir'\n",
        "adapter_upload_path = 'adapter_upload_path'\n",
        "adapter_save_path = 'adapter_save_path'\n",
        "domain_adapter_path = 'domain_adapter_path'\n",
        "\n",
        "lr = 0.0005\n",
        "num_epochs = 1"
      ],
      "metadata": {
        "id": "T3txzpHsSfbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "login(HF_KEY) # login to save checkpoints/models/adapters to HF hub\n",
        "set_seed(42)\n",
        "torch.manual_seed(42)\n",
        "qa_dataset = load_dataset(datasetpath, split=\"train\") # load dataset from HF hub\n",
        "qa_dataset.shuffle(42)\n",
        "qa_dataset = qa_dataset.train_test_split(test_size=0.15) # split dataset into train and test"
      ],
      "metadata": {
        "id": "P9ULkQgRTsYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load 4-bit quantized model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    modelpath,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    ),\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "uQ28ROENT2ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init(model) # when not using AdapterModel directly\n",
        "model.load_adapter(domain_adapter_path, load_as='domain', with_head=False) # load trained domain adapter either from local path or HF hub\n",
        "\n",
        "config = AdapterConfig.load(adapter_type, reduction_factor=r_factor)\n",
        "model.add_adapter(\"task\", config=config)\n",
        "model.train_adapter(\"task\")\n",
        "model.active_adapters = Stack('domain', 'task')\n",
        "\n",
        "model.adapter_to(device)\n",
        "\n",
        "print(model.adapter_summary())"
      ],
      "metadata": {
        "id": "96Y4C013UEbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# casting parameters for quantized setting\n",
        "for param in model.parameters():\n",
        "    if param.ndim == 1:\n",
        "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "        param.data = param.data.to(torch.float32)\n",
        "\n",
        "# if needed, to enable gradient checkpointing to reduce required memory\n",
        "# model.gradient_checkpointing_enable()\n",
        "# model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(torch.nn.Sequential):\n",
        "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "fD6naPXtU09B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset = qa_dataset.map(lambda x: {\"text\": f\"Answer the following question based on your biomedical knowledge: {x['input']} \\nAnswer: {x['output']}\"})\n",
        "\n",
        "def tokenize(element):\n",
        "    return tokenizer(element[\"text\"],\n",
        "                     truncation=True,\n",
        "                     max_length=256,\n",
        "                     add_special_tokens=False,)\n",
        "\n",
        "column_names = qa_dataset[\"train\"].column_names\n",
        "qa_data = qa_dataset.map(tokenize, batched=True, remove_columns=column_names)"
      ],
      "metadata": {
        "id": "GEWuvL8CVFOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=num_epochs,\n",
        "    logging_steps=1000,\n",
        "    #save_steps=500,\n",
        "    #eval_steps=187,\n",
        "    #save_total_limit=3,\n",
        "    #gradient_accumulation_steps=16,\n",
        "    #max_steps=1875,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    learning_rate=lr,\n",
        "    #group_by_length=True,\n",
        "    bf16=True,\n",
        "    report_to=\"none\",\n",
        "    #warmup_ratio=0.03,\n",
        "    #max_grad_norm=0.3,\n",
        ")\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        "    train_dataset = qa_data[\"train\"],\n",
        "    eval_dataset =  qa_data[\"test\"],\n",
        "    args=args,\n",
        ")"
      ],
      "metadata": {
        "id": "8o8UZ2oNWTUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "jD4-7MzXWm-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_res = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_res['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "tvbVRTPkWrGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_adapter_to_hub(adapter_upload_path, \"task\") # to upload adapter modules\n",
        "# model.save_adapter(adapter_save_path, 'task') # or saving locally"
      ],
      "metadata": {
        "id": "NzoJjeegWzM6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}